# -*- coding: utf-8 -*-
"""AI-Powered PDF Q&A Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_4i4zm923A7n7gXXTjVUanh5dhAA3P_
"""

!pip install langchain langchain-google-genai langchain-community google-generativeai PyPDF2 typesense python-dotenv

import os
from dotenv import load_dotenv

# Set Gemini API key here directly
os.environ["GOOGLE_API_KEY"] = "AIzaSyDYVhEFzolzdkviWfFqlxN-sBGgwfa3WOw"  # üîÅ Replace with your actual key

from PyPDF2 import PdfReader

# Load PDF file
pdfreader = PdfReader("/content/Fake_News_Detection_Using_Deep_Learning_A_Systematic_Literature_Review.pdf")  # üîÅ Replace with your actual filename

# Extract raw text
raw_text = ''
for page in pdfreader.pages:
    content = page.extract_text()
    if content:
        raw_text += content

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Use RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

# Create chunks
chunks = text_splitter.split_text(raw_text)

# Display the first few chunks to verify
print(f"Number of chunks: {len(chunks)}")
print("First chunk:")
print(chunks[0])

from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma

# Initialize Gemini Embedding
embeddings = GoogleGenerativeAIEmbeddings(
    model="models/embedding-001",  # You can also try "text-embedding-004" if allowed
    task_type="retrieval_document",
)

# Store in ChromaDB
# You can specify a directory for persistence, otherwise it's in-memory
# persist_directory = './chroma_db'
# docsearch = Chroma.from_texts(texts=chunks, embedding=embeddings, persist_directory=persist_directory)

docsearch = Chroma.from_texts(texts=chunks, embedding=embeddings)

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains.question_answering import load_qa_chain

# Initialize LLM
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash-latest", # Changed model name to a valid one
    convert_system_message_to_human=True
)

# Setup QA Chain
chain = load_qa_chain(llm, chain_type="stuff")

# Ask a question
question = "What is the main objective of this literature review?"

# Retrieve relevant documents
retriever = docsearch.as_retriever()
docs = retriever.invoke(question)

# Generate answer
answer = chain.invoke(input={"input_documents": docs, "question": question})
print("Answer:", answer)

